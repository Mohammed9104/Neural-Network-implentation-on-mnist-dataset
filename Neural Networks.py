# -*- coding: utf-8 -*-
"""nn week7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1okFfiMdmpta3aKOWHHfiQIDcr0DkNxYa
"""

import tensorflow as tf
import numpy as np

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Activation
import matplotlib.pyplot as plt
from tensorflow.keras.utils import to_categorical
import keras
from keras.layers import Conv2D, MaxPooling2D

(x_train,y_train),(x_test,y_test)=keras.datasets.mnist.load_data()

# Reshape the data to fit the model
x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)
x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
single_image = x_train[1, :].reshape(28,28)
plt.imshow(single_image)

model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(300, activation='relu'),
    keras.layers.Dense(100, activation='relu'),
    keras.layers.Dense(10, activation='softmax')]
                          )

"""model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(47, activation='softmax'))"""

# Compile the model
model.compile(loss='sparse_categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# Train the model
evaluate = model.fit(x_train, y_train ,epochs=10, validation_data=(x_test, y_test))

# Evaluate the model
score = model.evaluate(x_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

plt.plot(evaluate.history['accuracy'], label='accuracy')
plt.plot(evaluate.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0, 1])
plt.legend(loc='lower right')
plt.show()

weights,biases=model.layers[1].get_weights()
print ("First Hidden Layer Weights:",weights)
print("first Hidden Layer Weight dimentions",weights.shape)
print("first Hidden Layer biases ",biases)
print("first Hidden Layer biases dimentions",biases.shape)

model.summary()

weights,biases=model.layers[2].get_weights()
print ("Second Hidden Layer Weights:",weights)
print("Second Hidden Layer Weight dimentions",weights.shape)
print("Second Hidden Layer biases ",biases)
print("Second Hidden Layer biases dimentions",biases.shape)

optimizers=keras.optimizers.SGD(learning_rate=0.001,momentum=0.1)
model.compile(loss="sparse_categorical_crossentropy",
              optimizer=optimizers,
              metrics=["accuracy"])

evaluate = model.fit(x_train, y_train ,epochs=10, validation_data=(x_test, y_test))

# Evaluate the model
score = model.evaluate(x_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

my_callbacks=[
    tf.keras.callbacks.EarlyStopping(monitor='val_loss' '''loss''',patience=10),
    tf.keras.callbacks.ModelCheckpoint(filepath="MNIST_Model.h5"),
]

evaluate = model.fit(x_train, y_train ,batch_size=32,epochs=10, validation_data=(x_test, y_test),
callbacks=my_callbacks)
# Evaluate the model

print('Test loss:', score[0])
print('Test accuracy:', score[1])

plt.plot(evaluate.history['accuracy'], label='accuracy')
plt.plot(evaluate.history['loss'], label = 'loss')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0, 0.2])
plt.legend(loc='lower right')
plt.show()